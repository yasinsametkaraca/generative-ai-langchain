{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When we create a chatbot, we want it to take into account past conversations. In this way, he can give more natural and meaningful answers. In this notebook, we will create a chatbot considering past conversations.\n",
    "\n",
    "For this, we will create a new chain structure. This chain structure will keep the last input and conversation history."
   ],
   "id": "1a12517bda600c49"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:56.351766Z",
     "start_time": "2024-05-19T10:36:56.338126Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:56.397981Z",
     "start_time": "2024-05-19T10:36:56.378561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ],
   "id": "56db0999b9b9e0bb",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:56.888369Z",
     "start_time": "2024-05-19T10:36:56.398990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Our purpose in this notebook is to chat with a website.\n",
    "\n",
    "loader = WebBaseLoader(\"https://blog.langchain.dev/langgraph/\")\n",
    "docs = loader.load() # Load the documents from the website"
   ],
   "id": "ee275e1d31cb7365",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:56.903932Z",
     "start_time": "2024-05-19T10:36:56.889349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# We must chunk the documents to send them to the OpenAI API. Because the maximum token limit is 4096."
   ],
   "id": "6850d00d58b3f0a6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:57.947415Z",
     "start_time": "2024-05-19T10:36:56.905324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings() # Embedding model to convert the documents to vectors. Embedding means converting the text to a vector.\n",
    "# We will use the FAISS vector store to store the embeddings of the documents.\n",
    "vector_store = FAISS.from_documents(documents, embedding)"
   ],
   "id": "58ac13a841a9f87c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:57.962564Z",
     "start_time": "2024-05-19T10:36:57.949447Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vector_store.as_retriever() # retriever is a model that retrieves the most similar documents to a given query. ",
   "id": "98318862e28e6ece",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:57.978409Z",
     "start_time": "2024-05-19T10:36:57.963281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "# We use MessagesPlaceholder to set a place for the message history.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Here we actually keep the previous conversations.\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation generate a search query to look up in order to get information relevant to the conversation.\"), # Here we wrote this to help the model create a search query based on the conversation above.\n",
    "    ]\n",
    ")\n",
    "# The above prompt is a template for the chatbot. It will generate a search query based on the conversation history."
   ],
   "id": "131b1cfcf84604cb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's create a chain that retrieves the conversation history and returns the documents.",
   "id": "5c4dd5cfc51d4ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:57.993334Z",
     "start_time": "2024-05-19T10:36:57.979407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(\n",
    "    model,\n",
    "    retriever,\n",
    "    prompt,\n",
    ")\n",
    "# The above chain is a chain that retrieves the conversation history and returns the documents."
   ],
   "id": "87c7380f5bba6143",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's test the chain. We will create HumanMessage and AIMessage objects to test the chain.",
   "id": "b495abbbbdf32630"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:58.008505Z",
     "start_time": "2024-05-19T10:36:57.994617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Can I use LangGraph for my project?\"),\n",
    "    AIMessage(content=\"Yes, you can use LangGraph for your project.\"),\n",
    "]\n",
    "# above chat history is a conversation history."
   ],
   "id": "3c59a1f2226b9faa",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:59.400646Z",
     "start_time": "2024-05-19T10:36:58.009498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": \"Tell me how?\",\n",
    "    }\n",
    ")\n",
    "# The above code (retriever) will generate a search query based on the conversation history and return the documents."
   ],
   "id": "69aa6b1c66debc75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       " Document(page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\nBy LangChain\\n7 min read\\nJan 17, 2024', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       " Document(page_content=\"of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       " Document(page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll add a new chain to continue the conversation.",
   "id": "3d2105ad42a46af1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:59.416650Z",
     "start_time": "2024-05-19T10:36:59.402489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the user's questions based on the below context: \\n\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# The above prompt is a template for the chatbot. It will generate an answer based on the conversation history."
   ],
   "id": "8eb186f96b9e2c9f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:59.431803Z",
     "start_time": "2024-05-19T10:36:59.417648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# Create a chain for passing a list of Documents to a model\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(\n",
    "    model,\n",
    "    prompt,\n",
    ")"
   ],
   "id": "c2d8060c6868abf9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:36:59.447602Z",
     "start_time": "2024-05-19T10:36:59.432317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "# This method will receive the documents and forward them to the retrieval chain so that the model will respond according to the relevant document\n",
    "\n",
    "retriever_chain = create_retrieval_chain(\n",
    "    retriever_chain, # this retriever looks at past conversations and retrieves relevant documents.\n",
    "    documents_chain, # this chain passes the documents to the model.\n",
    ")"
   ],
   "id": "97ae077c7b1f2a44",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:37:04.641231Z",
     "start_time": "2024-05-19T10:36:59.448577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_history = [\n",
    "    HumanMessage(content=\"Can I use LangGraph for my project?\"),\n",
    "    AIMessage(content=\"Yes, you can use LangGraph for your project.\"),\n",
    "]\n",
    "\n",
    "response = retriever_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": \"Tell me how?\",\n",
    "    }\n",
    ")"
   ],
   "id": "dd060c8b5ccd07e6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:37:04.656700Z",
     "start_time": "2024-05-19T10:37:04.645190Z"
    }
   },
   "cell_type": "code",
   "source": "response",
   "id": "b347e499ecc3be0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can I use LangGraph for my project?'),\n",
       "  AIMessage(content='Yes, you can use LangGraph for your project.')],\n",
       " 'input': 'Tell me how?',\n",
       " 'context': [Document(page_content='TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for handling of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       "  Document(page_content='LangGraph\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph\\n\\nBy LangChain\\n7 min read\\nJan 17, 2024', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       "  Document(page_content=\"of more ambiguous inputs than simple chains. However, there is still an element of human guidance in terms of how that loop is constructed.LangGraph is a way to create these state machines by specifying them as graphs.FunctionalityAt it's core, LangGraph exposes a pretty narrow interface on top of LangChain.StateGraphStateGraph is a class that represents the graph. You initialize this class by passing in a state definition. This state definition represents a central state object that is updated over time. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store).The attributes of this state can be updated in two ways. First, an attribute could be overridden completely. This is useful if you want to nodes to return the new value of an attribute. Second, an attribute could be updated by adding to its value. This is useful if an attribute is a list of actions taken (or something similar) and you want nodes to return new actions taken (and have those automatically added to the attribute).You specify whether an attribute should be overridden or added to when creating the initial state definition. See an example in pseudocode below.from langgraph.graph import StateGraph\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'}),\n",
       "  Document(page_content='class State(TypedDict):\\n    input: str\\n    all_actions: Annotated[List[str], operator.add]\\n\\n\\ngraph = StateGraph(State)NodesAfter creating a StateGraph, you then add nodes with graph.add_node(name, value) syntax. The name parameter should be a string that we will use to refer to the node when adding edges. The value parameter should be either a function or LCEL runnable that will be called. This function/LCEL should accept a dictionary in the same form as the State object as input, and output a dictionary with keys of the State object to update.See an example in pseudocode below.graph.add_node(\"model\", model)\\ngraph.add_node(\"tools\", tool_executor)There is also a special END node that is used to represent the end of the graph. It is important that your cycles be able to end eventually!from langgraph.graph import ENDEdgesAfter adding nodes, you can then add edges to create the graph. There are a few types of edges.The Starting EdgeThis is the edge that connects the start of the graph to a particular node. This will make it so that that node is the first one called when input is passed to the graph. Pseudocode for that is:graph.set_entry_point(\"model\")Normal EdgesThese are edges where one node should ALWAYS be called after another. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool.graph.add_edge(\"tools\", \"model\")Conditional EdgesThese are where a function (often powered by an LLM) is used to determine which node to go to first. To create this edge, you need to pass in three things:The upstream node: the output of this node will be looked at to determine what to do nextA function: this will be called to determine which node to call next. It should return a stringA mapping: this mapping will be used to map the output of the function in (2) to another node. The keys should be possible values that the function in (2) could return. The values should be names of nodes to go to if that value is returned.An example of this could be that after a model is called we either exit the graph and return to the user, or we call a tool - depending on what a user decides! See an example in pseudocode below:graph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)CompileAfter we define our graph, we can compile it into a runnable! This simply takes the graph definition we\\'ve created so far an returns a runnable. This runnable exposes all the same method as LangChain runnables (.invoke, .stream, .astream_log, etc) allowing it to be called in the same manner as a chain.app = graph.compile()Agent ExecutorWe\\'ve recreated the canonical LangChain AgentExecutor with LangGraph. This will allow you to use existing LangChain agents, but allow you to more easily modify the internals of the AgentExecutor. The state of this graph by default contains concepts that should be familiar to you if you\\'ve used LangChain agents: input, chat_history, intermediate_steps (and agent_outcome to represent the most recent agent outcome)from typing import TypedDict, Annotated, List, Union\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.messages import BaseMessage\\nimport operator', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'title': 'LangGraph', 'language': 'en'})],\n",
       " 'answer': 'To use LangGraph for your project, you can follow these steps:\\n\\n1. Define a StateGraph: Initialize a StateGraph class by passing in a state definition that represents a central state object updated over time.\\n\\n2. Add Nodes: Use the graph.add_node(name, value) syntax to add nodes to the graph. Nodes should be functions or LCEL runnable that accept input and output dictionary with keys of the State object to update.\\n\\n3. Add Edges: Connect nodes by adding edges. There are three types of edges: Starting Edge (for the initial node), Normal Edges (to define the order of node execution), and Conditional Edges (for branching based on conditions).\\n\\n4. Compile: Once the graph is defined, you can compile it into a runnable using graph.compile(). This runnable can be called using standard LangChain runnable methods like .invoke, .stream, .astream_log, etc.\\n\\n5. Use Agent Executor: You can recreate the LangChain AgentExecutor using LangGraph to modify the internals of the AgentExecutor more easily. The state of this graph contains familiar concepts like input, chat_history, and intermediate_steps.\\n\\nBy following these steps, you can utilize LangGraph for your project to create cyclical graphs and agent runtimes.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T10:37:58.696451Z",
     "start_time": "2024-05-19T10:37:58.687835Z"
    }
   },
   "cell_type": "code",
   "source": "response[\"answer\"]",
   "id": "7ff7c46b9afe71dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To use LangGraph for your project, you can follow these steps:\\n\\n1. Define a StateGraph: Initialize a StateGraph class by passing in a state definition that represents a central state object updated over time.\\n\\n2. Add Nodes: Use the graph.add_node(name, value) syntax to add nodes to the graph. Nodes should be functions or LCEL runnable that accept input and output dictionary with keys of the State object to update.\\n\\n3. Add Edges: Connect nodes by adding edges. There are three types of edges: Starting Edge (for the initial node), Normal Edges (to define the order of node execution), and Conditional Edges (for branching based on conditions).\\n\\n4. Compile: Once the graph is defined, you can compile it into a runnable using graph.compile(). This runnable can be called using standard LangChain runnable methods like .invoke, .stream, .astream_log, etc.\\n\\n5. Use Agent Executor: You can recreate the LangChain AgentExecutor using LangGraph to modify the internals of the AgentExecutor more easily. The state of this graph contains familiar concepts like input, chat_history, and intermediate_steps.\\n\\nBy following these steps, you can utilize LangGraph for your project to create cyclical graphs and agent runtimes.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
